# Projeto ETL - Integra√ß√£o DummyJSON

Este reposit√≥rio cont√©m um pipeline de **ETL (Extract, Transform, Load)** desenvolvido em Python para demonstrar o consumo de APIs REST e a organiza√ß√£o de dados em camadas (Raw e Curated). O projeto foca na extra√ß√£o de dados de usu√°rios e produtos para posterior convers√£o e an√°lise.

## üèóÔ∏è Estrutura de Pastas

O projeto utiliza uma estrutura organizada para garantir a rastreabilidade dos dados em diferentes est√°gios de processamento:

* **raw/**: Camada de dados brutos, onde os arquivos `.json` s√£o persistidos exatamente como recebidos da API.
* **curated/**: Camada de dados transformados, onde os arquivos s√£o convertidos para `.csv` para facilitar o consumo em ferramentas de Analytics.
* **main.py**: Script principal que orquestra as fun√ß√µes de extra√ß√£o, carga e transforma√ß√£o.

## üõ†Ô∏è Tecnologias e Ferramentas

* **Linguagem**: Python.
* **Bibliotecas**: 
    * `Pandas` e `Numpy` para manipula√ß√£o e estrutura√ß√£o de dados.
    * `Requests` para integra√ß√£o com APIs REST.
* **Formato de Dados**: JSON (entrada) e CSV (sa√≠da).

## üöÄ Fluxo do Pipeline

O pipeline est√° dividido em tr√™s etapas principais:

1.  **Extra√ß√£o**: O script consome os endpoints da API `dummyjson.com`.
2.  **Carga (Raw)**: Os dados s√£o salvos localmente em formato JSON dentro da pasta `raw/`, respeitando o ID de cada registro.
3.  **Transforma√ß√£o (Curated)**: Os arquivos brutos s√£o lidos, processados via DataFrame e convertidos para CSV na pasta `curated/`.
